{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9aed9909-841f-4435-bfc6-3a04d9fffae2",
   "metadata": {},
   "source": [
    "# Quetion : 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31e67c80-ea46-4dcc-b35d-d47754184fc1",
   "metadata": {},
   "source": [
    " Random Forest Regressor is an ensemble learning method that combines multiple decision trees to perform regression tasks. It is an extension of the Random Forest algorithm, which is primarily used for classification tasks. The Random Forest Regressor builds a forest of decision trees, where each tree makes predictions on the input features, and the final prediction is obtained by aggregating the predictions of all trees."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20f9cb12-3617-4993-97c1-0eda164adf31",
   "metadata": {},
   "source": [
    "# Quetion : 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3622d397-65db-4982-bf69-aedc3f568c44",
   "metadata": {},
   "source": [
    "Random Forest Regressor reduces the risk of overfitting through two main mechanisms:\n",
    "\n",
    "Random feature selection: At each split in the decision tree, a random subset of features is considered for splitting. This randomization helps to decorrelate the trees and reduces the chance of overfitting to specific features in the dataset.\n",
    "Bootstrap sampling: The Random Forest Regressor uses bootstrap sampling, where each tree is trained on a random subset of the original training data. This introduces diversity in the training process and helps to mitigate the impact of outliers or noise in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05112cb6-58c1-4bd7-a6dc-421169fae7ee",
   "metadata": {},
   "source": [
    "# Quetion : 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ff06998-89f8-4b50-80a2-f1965848749b",
   "metadata": {},
   "source": [
    " Random Forest Regressor aggregates the predictions of multiple decision trees by taking the average (or weighted average) of the individual tree predictions. In regression tasks, the final prediction is obtained by averaging the predictions of all decision trees in the forest. This aggregation process helps to smooth out the individual tree predictions, reduce the variance, and provide a more stable and accurate prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab30e3f5-388d-48d5-abc3-335f432956d6",
   "metadata": {},
   "source": [
    "# Quetion : 4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c39eb709-5055-4abc-ae36-6697565f382e",
   "metadata": {},
   "source": [
    "Some of the important hyperparameters of the Random Forest Regressor include:\n",
    "\n",
    "n_estimators: The number of decision trees to be included in the forest.\n",
    "max_depth: The maximum depth allowed for each decision tree. It controls the complexity and potential overfitting of the individual trees.\n",
    "min_samples_split: The minimum number of samples required to split an internal node in a decision tree.\n",
    "max_features: The number of features to consider when looking for the best split at each node.\n",
    "bootstrap: A Boolean value indicating whether bootstrap sampling should be performed during the training process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cab4396-b9ef-4438-89ef-4b9b50878234",
   "metadata": {},
   "source": [
    "# Quetion : 5"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee020b27-d7f4-4dfe-87e3-4af52980a5eb",
   "metadata": {},
   "source": [
    "The main difference between Random Forest Regressor and Decision Tree Regressor is that Random Forest Regressor is an ensemble method, while Decision Tree Regressor is a single decision tree. Random Forest Regressor combines multiple decision trees, reducing overfitting and improving prediction accuracy. Decision Tree Regressor, on the other hand, builds a single decision tree that may be prone to overfitting and less capable of capturing complex relationships in the data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbffb359-d057-4b89-bdea-7da38d676bdd",
   "metadata": {},
   "source": [
    "# Quetion : 6"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d356834-4afb-434b-af9c-1d37d96f6d28",
   "metadata": {},
   "source": [
    "Advantages of Random Forest Regressor:\n",
    "\n",
    "Reduced risk of overfitting compared to a single decision tree.\n",
    "Improved prediction accuracy due to the aggregation of multiple decision trees.\n",
    "Robustness to outliers and noisy data.\n",
    "Can handle high-dimensional data with a large number of features.\n",
    "Provides feature importance rankings.\n",
    "Disadvantages of Random Forest Regressor:\n",
    "\n",
    "Increased complexity compared to a single decision tree.\n",
    "Requires more computational resources for training and prediction.\n",
    "The interpretation of the model can be challenging due to the ensemble nature."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1e3c00a-d80e-441c-a78b-5d5001b45411",
   "metadata": {},
   "source": [
    "# Quetion : 7"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9bc7ae7-c1c1-4b04-8937-0cf486650137",
   "metadata": {},
   "source": [
    "The output of Random Forest Regressor is a continuous numeric value, representing the predicted target variable for the given input features. It provides predictions for regression tasks, where the target variable is continuous and the goal is to estimate its value based on the input features."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "822e29c6-86fd-4f49-b50a-3d6f90a2f735",
   "metadata": {},
   "source": [
    "# Quetion : 8"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "417b299f-64a7-487e-823f-5c1778da9b20",
   "metadata": {},
   "source": [
    "Yes, Random Forest Regressor can also be used for classification tasks. However, the specific implementation for classification tasks is called Random Forest Classifier. While Random Forest Regressor predicts continuous values, Random Forest Classifier predicts discrete class labels. The main difference lies in the prediction task and the output type, with Random Forest Classifier providing class labels as the output."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
